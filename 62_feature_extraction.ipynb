{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Feature Extraction (FE)](https://scikit-learn.org/stable/modules/feature_extraction.html)\n",
    "\n",
    "- Used to extract feature information from text & image datasets.\n",
    "- Very different from [feature selection]() (FE is a technique that is applied to the result of a FE method.)\n",
    "\n",
    "### [Feactures from Dicts](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer)\n",
    "\n",
    "- [DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer) converts feature arrays (lists of Python `dict` objects) to NumPy/SymPy format.\n",
    "\n",
    "- Uses one-of-K (aka \"one hot\") category coding. Category features are unordered `attribute:value` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements = [\n",
    "    {'city': 'Dubai', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "    {'city': 'San Francisco', 'temperature': 18.},\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "vec.fit_transform(measurements).toarray()\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DictVectorizer accepts multiple strings for one feature (aka, multiple categories per movie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003},\n",
    "               {'category': ['animation', 'family'], 'year': 2011},\n",
    "               {'year': 1974}]\n",
    "\n",
    "vec.fit_transform(movie_entry).toarray()\n",
    "\n",
    "vec.get_feature_names() == ['category=animation', \n",
    "                            'category=drama',\n",
    "                            'category=family', \n",
    "                            'category=thriller',\n",
    "                            'year']\n",
    "\n",
    "vec.transform({'category': ['thriller'],\n",
    "               'unseen_feature': '3'}).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict Vectorizer - NLP applications\n",
    "\n",
    "- Suppose we have an algorithm that extracts Part of Speech (PoS) tags to use for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word ‘sat’ in the sentence ‘The cat sat on the mat.’:\n",
    "\n",
    "- The description can be vectorized into a sparse 2D matrix, suitable for a classifier.\n",
    "\n",
    "- Extracting this info around each individual word of a corpus of documents will return a *very wide (many one-hot-features) matrix* - with mostly zero values. `DictVectorizer` therefore uses a scipy.sparse matrix by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']\n"
     ]
    }
   ],
   "source": [
    "pos_window = [\n",
    "    {\n",
    "        'word-2': 'the',\n",
    "        'pos-2': 'DT',\n",
    "        'word-1': 'cat',\n",
    "        'pos-1': 'NN',\n",
    "        'word+1': 'on',\n",
    "        'pos+1': 'PP',\n",
    "    },\n",
    "    # in a real application one would extract many such dictionaries\n",
    "]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "pos_vectorized = vec.fit_transform(pos_window)\n",
    "print(pos_vectorized)\n",
    "\n",
    "pos_vectorized.toarray()\n",
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Feature Hashing](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher) \n",
    "\n",
    "- [FeatureHasher](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher) is a high-speed, low-memory vectorizer that uses a technique known as feature hashing, or the “hashing trick”. \n",
    "\n",
    "- Instead of building a hash table of features during training, as vectorizers do, instances of FeatureHasher *apply a hash function to the features to directly determine their column index* in sample matrices. \n",
    "\n",
    "- The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher *does not remember what the input features looked like* and has `no inverse_transform` method.\n",
    "\n",
    "- Since the hash function can cause collisions between (unrelated) features, a signed hash function is used. The sign determines the sign of the value stored in the output matrix for a feature. \n",
    "\n",
    "- This means that collisions are likely to cancel out rather than accumulate error - so the expected mean of any output feature’s value is zero. \n",
    "\n",
    "- It is enabled by default with `alternate_sign=True` and is particularly useful for small hash table sizes (n_features < 10000). For large hash table sizes, it can be disabled. This will allow outputs to be passed to estimators like MultinomialNB or chi2 feature selectors that expect non-negative inputs.\n",
    "\n",
    "- `FeatureHasher` accepts maps (like Python’s dict and its variants in the collections module), (feature, value) pairs, or strings, depending on the constructor parameter input_type. Maps are treated as lists of `(feature, value)` pairs. \n",
    "\n",
    "- Single strings have an implicit value of 1, so ['feat1', 'feat2', 'feat3'] is interpreted as [('feat1', 1), ('feat2', 1), ('feat3', 1)]. \n",
    "\n",
    "- If a single feature occurs multiple times in a sample, the feature values will be summed (so ('feat', 2) and ('feat', 3.5) become ('feat', 5.5)). The output from `FeatureHasher` is a scipy.sparse matrix in the CSR format.\n",
    "\n",
    "- Feature hashing can be used in document classification. Unlike `CountVectorizer`, `FeatureHasher` does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding. See below for a combined tokenizer/hasher.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- FeatureHasher uses the signed 32-bit variant of MurmurHash3. The maximum number of features supported is currently $2^31-1$.\n",
    "\n",
    "- The original formulation of the hashing trick used two separate hash functions $h$ and $phi$ to determine the column index and sign of a feature,. This implementation assumes the sign bit of MurmurHash3 is independent of its other bits.\n",
    "\n",
    "- Since a simple modulo is used to transform the hash function to a column index, consider using a power of two as the `n_features` param. Otherwise the features will not be mapped evenly to the columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:working] *",
   "language": "python",
   "name": "conda-env-working-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
