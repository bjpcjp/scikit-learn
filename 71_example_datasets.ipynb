{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dataset loader utilities](https://scikit-learn.org/stable/datasets.html)\n",
    "\n",
    "### Toy Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "# Boston house prices - 506 points, 13 attributes (none missing)\n",
    "X, y = load_boston(return_X_y=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Iris plants - 150 points (50/class, 3 classes), 4 attributes (none missing)\n",
    "data = load_iris()\n",
    "print(data.target[[10, 25, 50]])\n",
    "print(list(data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n"
     ]
    }
   ],
   "source": [
    "# diabetes (regression)\n",
    "# 442 samples, 10 attributes, integer targets\n",
    "X,y = load_diabetes(return_X_y=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# digits (classification)\n",
    "X,y = load_digits(return_X_y=True)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3) (20, 3)\n"
     ]
    }
   ],
   "source": [
    "# linnerud (regression)\n",
    "X,y = load_linnerud(return_X_y=True)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178,)\n"
     ]
    }
   ],
   "source": [
    "# wine (classification)\n",
    "X,y = load_wine(return_X_y=True)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n"
     ]
    }
   ],
   "source": [
    "# breast cancer (classification)\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real World (larger datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Olivetti Faces](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces)\n",
    "\n",
    "- 400 samples of face images, taken at AT&T 1992-94.\n",
    "- 4096 (64x64) dimensionality\n",
    "- feature values = quantized grey levels. loader\n",
    "- converts unsigned integers to floating point [0.0..1.0].\n",
    "- targets = integers [0..39] = person's identity\n",
    "- only 10 examples/class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30991736 0.3677686  0.41735536 ... 0.15289256 0.16115703 0.1570248 ]\n",
      " [0.45454547 0.47107437 0.5123967  ... 0.15289256 0.15289256 0.15289256]\n",
      " [0.3181818  0.40082645 0.49173555 ... 0.14049587 0.14876033 0.15289256]\n",
      " ...\n",
      " [0.5        0.53305787 0.607438   ... 0.17768595 0.14876033 0.19008264]\n",
      " [0.21487603 0.21900827 0.21900827 ... 0.57438016 0.59090906 0.60330576]\n",
      " [0.5165289  0.46280992 0.28099173 ... 0.35950413 0.3553719  0.38429752]]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  2  2  2  2\n",
      "  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4\n",
      "  4  4  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  7  7\n",
      "  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9\n",
      "  9  9  9  9 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11\n",
      " 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 14 14 14 14\n",
      " 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16\n",
      " 16 16 17 17 17 17 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 19 19\n",
      " 19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 21 21 21 21 21 21\n",
      " 21 21 21 21 22 22 22 22 22 22 22 22 22 22 23 23 23 23 23 23 23 23 23 23\n",
      " 24 24 24 24 24 24 24 24 24 24 25 25 25 25 25 25 25 25 25 25 26 26 26 26\n",
      " 26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 28 28 28 28 28 28 28 28\n",
      " 28 28 29 29 29 29 29 29 29 29 29 29 30 30 30 30 30 30 30 30 30 30 31 31\n",
      " 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33\n",
      " 33 33 33 33 34 34 34 34 34 34 34 34 34 34 35 35 35 35 35 35 35 35 35 35\n",
      " 36 36 36 36 36 36 36 36 36 36 37 37 37 37 37 37 37 37 37 37 38 38 38 38\n",
      " 38 38 38 38 38 38 39 39 39 39 39 39 39 39 39 39]\n"
     ]
    }
   ],
   "source": [
    "# olivetti faces (classification)\n",
    "faces = fetch_olivetti_faces()\n",
    "print(faces.data)\n",
    "print(faces.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 Newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)\n",
    "\n",
    "- 18K text posts on 20 topics, split into training & testing subsets\n",
    "- train/test split based on messages posted before/after a specified date\n",
    "- [loader #1](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups): returns list of raw texts for feeding to feature extractors\n",
    "- [loader #2](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized) returns ready-to-use features (no extractor needed)\n",
    "- Downloads data, extracts to `~/scikit_learn_data/20news_home`, then calls [load_files]() on training and/or test data folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# 20 newsgroups text corpus (20 classes, 18.8K samples)\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "from pprint import pprint\n",
    "pprint(list(data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314,)\n",
      "(11314,)\n",
      "[ 7  4  4  1 14 16 13  3  2  4]\n"
     ]
    }
   ],
   "source": [
    "print(data.filenames.shape)\n",
    "print(data.target.shape)\n",
    "print(data.target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting text to TF-IDF vectors of unigram tokens\n",
    "- vectors should be very sparse: ~159 non-zero values in a 30K dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 34118)\n",
      "159.0132743362832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "categories = ['alt.atheism', 'talk.religion.misc',\n",
    "              'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      categories=categories)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(vectors.shape)\n",
    "print(vectors.nnz/float(vectors.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *It's easy to overfit a classifier* on irrelevant Newsgroup data such as headers.\n",
    "- Example: Multinomial Naive Bayes - fast to train, decent F-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8821359240272957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "vectors_test    = vectorizer.transform(newsgroups_test.data)\n",
    "clf             = MultinomialNB(alpha=.01).fit(vectors, \n",
    "                                               newsgroups_train.target)\n",
    "\n",
    "pred = clf.predict(vectors_test)\n",
    "print(metrics.f1_score(newsgroups_test.target, \n",
    "                       pred, \n",
    "                       average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most informative features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: edu it and in you that is of to the\n",
      "comp.graphics: edu in graphics it is for and of to the\n",
      "sci.space: edu it that is in and space to of the\n",
      "talk.religion.misc: not it you in is that and to of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    \n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "show_top10(clf, vectorizer, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Things that cause overfit:\n",
    "    - Headers: \"NNTP-Posting-Host:\", \"Distribution:\"\n",
    "    - Whether the sender is affiliated with a university (headers, signatures)\n",
    "    - The word \"article\"\n",
    "\n",
    "\n",
    "- These clues make newsgroup classification much easier. To make the job harder, use  `remove` to strip out information. (params can be a combination of 'headers','footers','quotes')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.870270086666271\n",
      "0.8819288688799596\n",
      "0.8401551094938573\n",
      "0.7731035068127478\n"
     ]
    }
   ],
   "source": [
    "newsgroups_test_nh = fetch_20newsgroups(subset='test', remove=('headers'),\n",
    "                                        categories=categories)\n",
    "newsgroups_test_nf = fetch_20newsgroups(subset='test', remove=('footers'),\n",
    "                                        categories=categories)\n",
    "newsgroups_test_nq = fetch_20newsgroups(subset='test', remove=('quotes'),\n",
    "                                        categories=categories)\n",
    "newsgroups_test_all = fetch_20newsgroups(subset='test', remove=('headers',\n",
    "                                                                'footers',\n",
    "                                                                'quotes'),\n",
    "                                        categories=categories)\n",
    "\n",
    "vectors_test_nh = vectorizer.transform(newsgroups_test_nh.data)\n",
    "vectors_test_nf = vectorizer.transform(newsgroups_test_nf.data)\n",
    "vectors_test_nq = vectorizer.transform(newsgroups_test_nq.data)\n",
    "vectors_test_all = vectorizer.transform(newsgroups_test_all.data)\n",
    "\n",
    "pred_nh = clf.predict(vectors_test_nh)\n",
    "pred_nf = clf.predict(vectors_test_nf)\n",
    "pred_nq = clf.predict(vectors_test_nq)\n",
    "pred_all = clf.predict(vectors_test_all)\n",
    "\n",
    "print(metrics.f1_score(pred_nh, newsgroups_test_nh.target, average='macro'))\n",
    "print(metrics.f1_score(pred_nf, newsgroups_test_nf.target, average='macro'))\n",
    "print(metrics.f1_score(pred_nq, newsgroups_test_nq.target, average='macro'))\n",
    "print(metrics.f1_score(pred_all, newsgroups_test_all.target, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Faces in the Wild\n",
    "\n",
    "- Collection of .jpegs; each picture is centered on a single face.\n",
    "- 5.7K classes, 13.2K samples, 5.8K dimensions, 0-255 data\n",
    "- Dataset size >200MB\n",
    "- Loader downloads to `~/scikit_learn_data/lfw_home/` using `joblib`, parses metadata, decodes jpegs, converts slices into memmapped numpy arrays.\n",
    "- The first loader is for face ID (a multiclass classification task).\n",
    "- Default slice = rectangular shape around face (most background removed).\n",
    "- Each face assigned to single person in `target`.\n",
    "- Second loader is for face verification - each sample is a 2-pic pair, belonging (or not) to the same person.\n",
    "- Both loaders can add RGB color info with `color=True`.\n",
    "- Datasets are divided into `train`,`test` and `10_folds` evaluation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 \t Ariel Sharon\n",
      "float32\n",
      "(1288, 1850)\n",
      "(1288, 50, 37)\n"
     ]
    }
   ],
   "source": [
    "data = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "print(data.target_names.size,\"\\t\",data.target_names[0])\n",
    "print(data.data.dtype)\n",
    "print(data.data.shape)\n",
    "print(data.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Different persons' 'Same person']\n",
      "(2200, 2, 62, 47)\n",
      "(2200, 5828)\n",
      "(2200,)\n"
     ]
    }
   ],
   "source": [
    "pairs = fetch_lfw_pairs(subset='train')\n",
    "print(pairs.target_names)\n",
    "print(pairs.pairs.shape)\n",
    "print(pairs.data.shape)\n",
    "print(pairs.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Forest Covertypes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype)\n",
    "\n",
    "- Describes dominant tree species for 30x30m patches of US forestry.\n",
    "- 581K samples.\n",
    "- Seven possible values (multiclass)\n",
    "- 54 features (some boolean, some discrete, some continuous.\n",
    "- Default: returns `data` (a dict-like 'Bunch' object) with feature matrix & `target`.\n",
    "- `as_frame=True`: returns as a Pandas dataframe instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 54)\n",
      "(581012,)\n",
      "[2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
      " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "data = fetch_covtype()\n",
    "print(data.data.shape)\n",
    "print(data.target.shape)\n",
    "print(data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Reuters Newswire Corpus](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_rcv1.html#sklearn.datasets.fetch_rcv1)\n",
    "\n",
    "- 103 classes\n",
    "- 804K samples with 47K dimensions, [0..1] values. Compressed dataset size ~656MB.\n",
    "- Returns a dict-like object with:\n",
    "    - `data` is a scipy sparse CSR matrix. Non-zero values are cosine-normalized TF-IDF vectors; first 23K samples = training, last 781K samples = test. Array should have 0.16% non-zero values.\n",
    "    - `target` is a scipy sparse CSR matrix. 804K samples, 103 categories. Each sample has 1 in its category, 0 in the others. 3.1% non-zero.\n",
    "    - each sample can be identified by its ID#.\n",
    "    - `target_names` are the topics of each sample (1 to 17). 103 topics (strings). Corpus frequencies range from 5 ('GMIL') to 381K ('CCAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(804414, 47236)\n",
      "(804414, 103)\n",
      "[2286 2287 2288]\n",
      "['C11', 'C12', 'C13', 'C14', 'C15', 'C151', 'C1511', 'C152', 'C16', 'C17']\n"
     ]
    }
   ],
   "source": [
    "data = fetch_rcv1()\n",
    "print(data.data.shape)\n",
    "print(data.target.shape)\n",
    "print(data.sample_id[:3])\n",
    "print(data.target_names[:10].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [KDD CUP (1998 DARPA intrusion detection eval dataset0]()\n",
    "\n",
    "- Artificial data with hand-injected attacks.\n",
    "- Original dataset has ~80% of abnormal data = unrealistic for anomaly detection, so dataset is splite into **SA** && **SF**.\n",
    "    - SA selects all normal data + ~1% of abnormal data.\n",
    "    - SF selects all data with `logged_in` = positive (focuses on intrusion attack ~ 0.3%)\n",
    "    - `http` and `smtp` are subsets of SF.\n",
    "    \n",
    "- 4.89M samples, 41 dimensions, discrete/continuous data\n",
    "- targets: 'normal' or anomaly name\n",
    "- SA: 976K samples, 41 dimensions; \n",
    "- SF: 699K samples, 4 dimensions\n",
    "- http: 619K samples, 3 dimensions\n",
    "- smtp: 95K samples, 3 dimensions\n",
    "\n",
    "- returns `data` feature matrix, `target` values. `as_frame=True` returns `data` as a Pandas dataframe and `target` as a Pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9571, 3)\n",
      "(9571,)\n",
      "[-2.3025850929940455 8.12151008316269 5.796361655949294]\n"
     ]
    }
   ],
   "source": [
    "data = fetch_kddcup99(subset='smtp')\n",
    "print(data.data.shape)\n",
    "print(data.target.shape)\n",
    "print(data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [California Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing)\n",
    "\n",
    "- 20K samples, 8 attributes, no missing attribute values\n",
    "- Measures median house value for California districts, 1990 census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n",
      "[   8.3252       41.            6.98412698    1.02380952  322.\n",
      "    2.55555556   37.88       -122.23      ]\n"
     ]
    }
   ],
   "source": [
    "data = fetch_california_housing()\n",
    "print(data.data.shape)\n",
    "print(data.target.shape)\n",
    "print(data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:working] *",
   "language": "python",
   "name": "conda-env-working-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
