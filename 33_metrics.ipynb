{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Metrics/Scoring](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "- Three Scikit APIs of note:\n",
    "    - **estimator `score` methods**: provides a default evaluation method.\n",
    "    - **scoring parameter**: used by Cross Validation tools.\n",
    "    - **metric functions**: used to compute prediction error in specific situations.\n",
    "    \n",
    "- [dummy estimators](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators) provide baseline metric values for random predictions.\n",
    "\n",
    "- [pairwise metrics](https://scikit-learn.org/stable/modules/metrics.html#metrics) provide metrics *between samples*. They are not estimators or predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scoring` - defines model evaluation rules\n",
    "\n",
    "- Model selection & evaluation tools (Grid Search, cross_val_score) use `scoring` to decide which metric to apply to an estimator.\n",
    "\n",
    "- functions ending in `_score`: higher values = \"better\".\n",
    "- functions ending with `_error` or `_loss`: lower values = \"better\".\n",
    "\n",
    "- Sometimes you'll need to wrap a scoring function with `make_scorer` to make it \"callable\" for model evaluations. The `fbeta_score` method is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "grid = GridSearchCV(LinearSVC(), \n",
    "                    param_grid = {'C': [1, 10]},\n",
    "                    scoring    = ftwo_scorer, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Scoring functions with `make_scorer`\n",
    "\n",
    "- Build a custom scoring object with these components:\n",
    "    - a python function\n",
    "    - whether the function returns a *score* or a *loss* (`greater_is_better` is True (default) or False respectively.)\n",
    "    - whether the function requires *continuous decision certainties* (`needs_threshold=True`). (For classification metrics only.)\n",
    "    - any additional params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    diff = np.abs(y_true - y_pred).max()\n",
    "    return np.log1p(diff)\n",
    "\n",
    "# score will negate the return value of my_custom_loss_func,\n",
    "# which will be np.log(2), 0.693, given the values for X\n",
    "# and y defined below.\n",
    "\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "X,y   = [[1], [1]], [0, 1]\n",
    "clf   = DummyClassifier(strategy='most_frequent', \n",
    "                        random_state=0).fit(X, y)\n",
    "\n",
    "my_custom_loss_func(y, clf.predict(X))\n",
    "score(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple metrics\n",
    "\n",
    "- Available in `GridSearchCV`, `RandomizedSearchCV`, `cross_validate`. Multiple ways to specify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specified as iterable of strings\n",
    "scoring = ['accuracy', 'precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specified as a dict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'prec': 'precision'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  9  8  7  8]\n",
      "[0 1 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "# specified as a callable\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "     y_pred = clf.predict(X)\n",
    "     cm = confusion_matrix(y, y_pred)\n",
    "     return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "             'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "    \n",
    "X, y = make_classification(n_classes=2, \n",
    "                                    random_state=0)\n",
    "svm = LinearSVC(random_state=0)\n",
    "\n",
    "cv_results = cross_validate(svm, X, y, cv=5,\n",
    "                            scoring=confusion_matrix_scorer)\n",
    "\n",
    "print(cv_results['test_tp'])\n",
    "print(cv_results['test_fn'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:working] *",
   "language": "python",
   "name": "conda-env-working-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
